/*
 * Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the "Software"),
 * to deal in the Software without restriction, including without limitation
 * the rights to use, copy, modify, merge, publish, distribute, sublicense,
 * and/or sell copies of the Software, and to permit persons to whom the
 * Software is furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in
 * all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
 * DEALINGS IN THE SOFTWARE.
 */
#version 450 core
#pragma use_vulkan_memory_model
#extension GL_KHR_shader_subgroup_basic : enable
#extension GL_EXT_scalar_block_layout : enable
#extension GL_KHR_memory_scope_semantics : enable
#extension GL_NV_cooperative_matrix : enable
#extension GL_NV_integer_cooperative_matrix : enable
#extension GL_EXT_shader_explicit_arithmetic_types_float16 : enable
#extension GL_EXT_shader_explicit_arithmetic_types_int8 : enable
#extension GL_EXT_shader_explicit_arithmetic_types_int32 : enable
#extension GL_EXT_buffer_reference : enable
#extension GL_EXT_control_flow_attributes : enable

// M/N/K values filled out at pipeline creation time
layout(constant_id = 0) const uint lM = 1;
layout(constant_id = 1) const uint lN = 1;
layout(constant_id = 2) const uint lK = 1;
layout(constant_id = 3) const uint TILE_M = 1;
layout(constant_id = 4) const uint TILE_N = 1;
layout(constant_id = 5) const uint TILE_K = 1;
layout(constant_id = 6) const uint K = 1;
layout(constant_id = 7) const uint strideA = 1;
layout(constant_id = 8) const uint strideB = 1;
layout(constant_id = 9) const uint strideC = 1;
layout(constant_id = 10)const uint strideD = 1;
layout(constant_id = 11)const float alpha = 1.0;
layout(constant_id = 12)const float beta = 1.0;
layout(constant_id = 13)const bool BColMajor = false;

// #defines set on command line:
// A_BITS = 8 or 16 or 32 (bits per component)
// A_TYPE = e.g. float or float16_t
// C_BITS = 8 or 16 or 32 (bits per component)
// C_TYPE = e.g. float or float16_t
// coopmatT = fcoopmatNV, ucoopmatNV, scoopmatNV

layout(buffer_reference) buffer InputA { A_TYPE x[]; } inputA;
layout(buffer_reference) buffer InputB { A_TYPE x[]; } inputB;
layout(buffer_reference) buffer InputC { C_TYPE x[]; } inputC;
layout(buffer_reference) buffer Output { C_TYPE x[]; } outputO;
layout(set=0, binding=0, std430) uniform Params { InputA inputA; InputB inputB; InputC inputC; Output outputO; } params;
layout(local_size_x = 32, local_size_y = 1, local_size_z = 1) in;

const uint C_ROWS = TILE_M / lM;
const uint C_COLS = TILE_N / lN;
coopmatT<C_BITS, gl_ScopeSubgroup, lM, lN> result[C_ROWS][C_COLS];

uint coordToOffset(uint i, uint j, uint stride, bool colMajor)
{
    return colMajor ? (stride * j + i) : (stride * i + j);
}

void main()
{
    uvec2 tileID = uvec2(gl_WorkGroupID.xy);

    InputA inputA = params.inputA;
    InputB inputB = params.inputB;
    InputC inputC = params.inputC;
    Output outputO = params.outputO;

    // Initialize result to zero
    [[unroll]] for (uint i = 0; i < C_ROWS; ++i) {
        [[unroll]] for (uint j = 0; j < C_COLS; ++j) {
            result[i][j] = coopmatT<C_BITS, gl_ScopeSubgroup, lM, lN>(0.0);
        }
    }

    // On each iteration, load a row of cooperative matrices from matrix A,
    // load a column of cooperative matrices from matrix B, and multiply all
    // pairs of those matrices.
    for (uint chunkK = 0; chunkK < K; chunkK += TILE_K) {
        coopmatT<A_BITS, gl_ScopeSubgroup, lM, lK> matA[C_ROWS];
        [[unroll]] for (uint i = 0; i < C_ROWS; ++i) {
            uint gi = TILE_M * tileID.y + lM * i;
            uint gk = chunkK;
            coopMatLoadNV(matA[i], inputA.x, coordToOffset(gi, gk, strideA, false), strideA, false);
        }
        coopmatT<A_BITS, gl_ScopeSubgroup, lK, lN> matB;
        [[unroll]] for (uint j = 0; j < C_COLS; ++j) {
            uint gj = TILE_N * tileID.x + lN * j;
            uint gk = chunkK;
            coopMatLoadNV(matB, inputB.x, coordToOffset(gk, gj, strideB, BColMajor), strideB, BColMajor);
            [[unroll]] for (uint i = 0; i < C_ROWS; ++i) {
                result[i][j] = coopMatMulAddNV(matA[i], matB, result[i][j]);
            }
        }
    }

    [[unroll]] for (uint i = 0; i < C_ROWS; ++i) {
        [[unroll]] for (uint j = 0; j < C_COLS; ++j) {
            uint gi = TILE_M * tileID.y + lM * i;
            uint gj = TILE_N * tileID.x + lN * j;

            // fetch and add C matrix
            coopmatT<C_BITS, gl_ScopeSubgroup, lM, lN> matC;
            coopMatLoadNV(matC, inputC.x, coordToOffset(gi, gj, strideC, false), strideC, false);

            result[i][j] = C_TYPE(alpha) * result[i][j] + C_TYPE(beta) * matC;
            coopMatStoreNV(result[i][j], outputO.x, coordToOffset(gi, gj, strideD, false), strideD, false);
        }
    }
}
